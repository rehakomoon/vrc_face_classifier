{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import FaceInferenceImageDataset, FaceDataset, PyLModel\n",
    "from utils import ScaleCategory, find_latest_checkpoint_path, load_inference_dir\n",
    "\n",
    "data_dir = Path.cwd() / \"data_scale\"\n",
    "dataset_dir = data_dir / \"dataset\"\n",
    "log_dir = data_dir / \"log\"\n",
    "inference_dir = data_dir / \"inference\"\n",
    "#inference_dir = data_dir / \"raw\"\n",
    "output_dir = data_dir / \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "time.sleep(1)\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "#device = \"cpu\"\n",
    "\n",
    "batch_size=512\n",
    "#batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceInferenceImageDataset(inference_dir)\n",
    "dataset = FaceDataset(dataset, with_flipped=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "\n",
    "checkpoint_path = find_latest_checkpoint_path(log_dir / \"lightning_logs\")\n",
    "assert(checkpoint_path is not None)\n",
    "\n",
    "model = PyLModel.load_from_checkpoint(str(checkpoint_path))\n",
    "print(\"Load:\", checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_input_dir = data_dir / \"tmp_input\"\n",
    "tmp_output_dir = data_dir / \"tmp_output\"\n",
    "\n",
    "shutil.rmtree(tmp_input_dir, ignore_errors=True)\n",
    "shutil.rmtree(tmp_output_dir, ignore_errors=True)\n",
    "time.sleep(1)\n",
    "tmp_input_dir.mkdir(exist_ok=True)\n",
    "tmp_output_dir.mkdir(exist_ok=True)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = load_inference_dir(inference_dir)\n",
    "parameters = [(p, tmp_input_dir / p.name) for p in parameters]\n",
    "\n",
    "print(\"copy files:\", len(parameters))\n",
    "\n",
    "for src_path, dst_path in tqdm(parameters):\n",
    "    shutil.copyfile(src_path, dst_path)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 0.1\n",
    "\n",
    "def resize_process(param):\n",
    "    src_path, dst_path = param\n",
    "\n",
    "    image = cv2.imread(str(src_path), cv2.IMREAD_COLOR)\n",
    "    image_h, image_w, _ = image.shape\n",
    "\n",
    "    trim_w = math.ceil(image_w * scale_factor * 0.5)\n",
    "    trim_h = math.ceil(image_h * scale_factor * 0.5)\n",
    "\n",
    "    crop_image = image[trim_h:image_h-trim_h-1, trim_w:image_w-trim_w-1, :]\n",
    "    crop_image = cv2.resize(crop_image, (image_w, image_h), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    cv2.imwrite(str(dst_path), crop_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scale in range(64):\n",
    "    dataset = FaceInferenceImageDataset(tmp_input_dir)\n",
    "    if (len(dataset) == 0):\n",
    "        break\n",
    "    \n",
    "    dataset = FaceDataset(dataset, with_flipped=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    estimated_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch, batch_flip, _ = batch\n",
    "            this_batch_size = len(batch)\n",
    "\n",
    "            batch = torch.cat([batch, batch_flip], dim=0)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            pred = model(batch)\n",
    "\n",
    "            pred = pred.view(2, this_batch_size, len(ScaleCategory))\n",
    "            pred = pred.sum(dim=0)\n",
    "            _, estimated = pred.max(dim=1)\n",
    "\n",
    "            estimated = estimated.cpu().numpy()\n",
    "            estimated_list.append(estimated)\n",
    "\n",
    "    estimated_list = np.concatenate(estimated_list, axis=0)\n",
    "    \n",
    "    image_files = dataset.dataset.paths\n",
    "    assert(len(estimated_list) == len(image_files))\n",
    "    \n",
    "    output_dir_selector = {\n",
    "        ScaleCategory.valid: tmp_output_dir,\n",
    "        ScaleCategory.invalid: output_dir,\n",
    "    }\n",
    "    \n",
    "    parameters = zip(image_files, estimated_list)\n",
    "    parameters = list(parameters)\n",
    "    parameters = [(p, output_dir_selector[l] / p.name) for p, l in parameters]\n",
    "    \n",
    "    shutil.rmtree(tmp_output_dir, ignore_errors=True)\n",
    "    time.sleep(1)\n",
    "    tmp_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for src_path, dst_path in tqdm(parameters):\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "    \n",
    "    shutil.rmtree(tmp_input_dir, ignore_errors=True)\n",
    "    time.sleep(1)\n",
    "    tmp_input_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    parameters = list(tmp_output_dir.glob(\"*.png\"))\n",
    "    parameters = [(p, tmp_input_dir / f\"{p.stem}_.png\") for p in parameters]\n",
    "    \n",
    "    #[resize_process(params) for params in tqdm(parameters)]\n",
    "    Parallel(n_jobs=-1, verbose=10)([delayed(resize_process)(params) for params in parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = load_inference_dir(output_dir)\n",
    "output_files = [p.stem.split(\"_\") for p in output_files]\n",
    "assert(min([len(l) for l in output_files]) >= 4)\n",
    "output_files = [\"_\".join(l[0:4]) for l in output_files]\n",
    "input_files = load_inference_dir(inference_dir)\n",
    "input_files = [p.stem for p in input_files]\n",
    "\n",
    "parameters = [f\"{p}.png\" for p in tqdm(input_files) if not p in output_files]\n",
    "parameters = [(inference_dir / p, output_dir / p) for p in parameters]\n",
    "\n",
    "for src_path, dst_path in tqdm(parameters):\n",
    "    shutil.copyfile(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
