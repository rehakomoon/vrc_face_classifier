{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import FaceInferenceImageDataset, FaceDataset, PyLModel, load_inference_dir\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "raw_dir = Path.cwd() / \"data_scale/raw\"\n",
    "input_dir = Path.cwd() / \"data_scale/output\"\n",
    "output_dir = Path.cwd() / \"data_filter/raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "time.sleep(1)\n",
    "output_dir.parent.mkdir(exist_ok=True)\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = load_inference_dir(input_dir)\n",
    "parameters = [(p.name, p.stem.split(\"_\")) for p in parameters]\n",
    "assert(min([len(l) for _, l in parameters]) >= 4)\n",
    "parameters = [(p, \"_\".join(l[0:4]), len(l)-4) for p, l in parameters]\n",
    "parameters = [(raw_dir / (src_stem + \".png\"), output_dir / dst_filename, scale) for dst_filename, src_stem, scale in parameters]\n",
    "print(max([s for _, _, s in parameters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 0.1\n",
    "estimation_margin = 2\n",
    "\n",
    "def process(params):\n",
    "    src_path, dst_path, scale = params\n",
    "\n",
    "    scale = max(0, scale - estimation_margin)\n",
    "    print(src_path)\n",
    "\n",
    "    image = cv2.imread(str(src_path), cv2.IMREAD_COLOR)\n",
    "    image_h, image_w, _ = image.shape\n",
    "\n",
    "    remove_ratio = 1.0 - (1.0 - scale_factor) ** scale\n",
    "    trim_w = math.ceil(image_w * remove_ratio * 0.5)\n",
    "    trim_h = math.ceil(image_h * remove_ratio * 0.5)\n",
    "\n",
    "    crop_image = image[trim_h:image_h-trim_h-1, trim_w:image_w-trim_w-1, :]\n",
    "    crop_image = cv2.resize(crop_image, (image_w, image_h), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    if (False):\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        plt.imshow(cv2.cvtColor(crop_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "\n",
    "    cv2.imwrite(str(dst_path), crop_image)\n",
    "\n",
    "#[process(params) for params in tqdm(parameters)]\n",
    "Parallel(n_jobs=-1, verbose=10)([delayed(process)(params) for params in parameters])\n",
    "print(\"output done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
